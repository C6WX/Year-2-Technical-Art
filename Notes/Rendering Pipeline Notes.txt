Rendering Pipeline Notes

A rendering pipeline is a sequence of operations that take the contents of a scene and display them on-screen. The pipeline transforms 3D models into 2D images through a series of calculations.

Rendering pipeline stages:
1. Input assembly 
2. Vertex Processing
3. Rasterisation
4. Fragment/Pixel processing
5. Post Processing

Draw Calls 
The CPU gathers all the data from a scene for rendering. This data includes Vertex buffers, Index buffers, Shader Programs, Uniform Buffers/Textures and State Objects. The CPU sends all the data through a series of API calls, which vary based on the graphics API used, and then to the GPU. The CPU is always processing one frame ahead of the GPU. 
Draw calls are created when there are unique meshes, materials, dynamic objects and lights in a scene.


Input assembly
After receiving Draw call’s from the CPU the GPU enters the input Assembly (IA) stage which is responsible for gathering all the data sent by the draw call: vertex data containing the raw positions of vertices, their attributes (colors, normals, etc) and any other information and organizing this data into primitives based on the specified primitive type (e.g. Triangle, line, or point


Vertex Processing
After the Input Assembly groups raw vertices into primitives, the GPU sends each vertex to the Vertex Processing Stage

-this is where each individual vertex gets processed (in parallel on the GPU).
-Each vertex starts in object (relative/local/modal) space
-The vertex shader then multiplies it by matrices to move it through spaces:
worldPosition=Model×vertexPosition | viewPosition=View×worldPosition | clipPosition=Projection×viewPosition |

clipPosition=Projection×View×Model×vertexPosition

This is known as the MVP matrix chain, which is a series of matrix calculations to get each vertex moved on to the screen space, accounting for where the model is in space, where the camera is, and the perspective angle from the camera


Rasterisation
After vertex processing transforms all vertices into screen space, the Rasterisation stage determines which pixels on the screen are covered by each primitive (triangle, line, or point).

Primitive Assembly: Vertices are grouped back into their original primitives
Clipping: Primitives outside the view frustum are removed or clipped
Triangle Setup: Calculate edge equations and interpolation parameters

Scan Conversion: Determine which pixels are inside each triangle
Attribute Interpolation: Smoothly blend vertex attributes (colours, UVs, normals) across the surface

Rasterization process
Edge Function Testing: Mathematical tests determine if a pixel center lies inside the triangle

-Barycentric Coordinates: Calculate weights for interpolating vertex attributes
-Fragment Generation: Each covered pixel becomes a “fragment” - a potential pixel with interpolated data
-Early Z-Testing: Some GPUs perform depth testing here to discard occluded fragments early
-Fragment/Pixel processing


Fragment/Pixel processing
The Fragment Shader (also called Pixel Shader) processes each fragment generated by rasterization. This is where the final colour and appearance of each pixel is determined.

Fragments contain interpolated vertex data (position, colour, UV coordinates, normals)
Fragment shaders run in parallel across hundreds or thousands of GPU cores
This stage handles lighting calculations, texture sampling, and material properties
Output is typically an RGBA colour value and depth
Texture Sampling: Reading colour/data from texture maps using UV coordinates
Lighting Calculations: Applying lighting models (Phong, Blinn-Phong, PBR)

Normal Mapping: Using normal maps to add surface detail
Material Properties: Applying roughness, metallic, and specular properties


Post-Processing
Post-processing occurs after the main scene has been rendered to a framebuffer. These effects are applied as full-screen passes, treating the rendered image as a texture.

Common Post-Processing Effects:
-Tone Mapping & Gamma Correction
-Bloom & HDR
-Anti-Aliasing (FXAA, TAA)
-Depth of Field
-Motion Blur
-Colour Grading


Terminology -
Vertex buffers: The CPU organises the 3D geometry (positions, normals, colours, texture coordinates, Material IDs) into vertex buffer which are later processed by the GPU.

Index buffers: If indexed drawing is used, the CPU prepares an index buffer that specifies how to assemble vertices into primitives (e.g how to form triangles and vertex positions)

Shader Programs: The CPU loads and compiles vertex, fragment, and possibly geometry shaders into the GPU’s memory. These shaders define how the vertex data is transformed and how pixels are shaded.

Uniform Buffers/Textures: The CPU also sends any global data that shaders might need, such as transformation matrices (e.g., model, view, projection matrices), lighting information, and texture data.

State Objects: The CPU also configures the GPU state, like which shader program to use, which textures to bind, and how to handle blending, depth testing, and rasterisation.

Dynamic objects:  Objects that move or change often.